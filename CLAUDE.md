# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**ðŸš¨ CONSTITUTION COMPLIANCE MANDATORY**: All development must follow the principles in `.specify/memory/constitution.md`.

## Development Commands

### Running the Application

- **Development with reload**: `python -m uvicorn app.main:app --host 0.0.0.0 --port 5681 --reload`
- **Production with pm2**: `pm2 start "uvicorn app.main:app --host 0.0.0.0 --port 5681" --name scrapling-api`

### Testing

- **All tests**: `python -m pytest`
- **Quick test run**: `python -m pytest -q`
- **Verbose output**: `python -m pytest -v`
- **Unit tests only**: `python -m pytest -m unit`
- **Integration tests only**: `python -m pytest -m integration` (requires real network/browser)

### Test Structure

The test suite is organized into two main categories:

```
tests/
â”œâ”€â”€ unit/           # Unit tests (mocked, no network/browser dependencies)
â”‚   â”œâ”€â”€ api/        # API endpoint unit tests
â”‚   â”œâ”€â”€ core/       # Configuration, logging unit tests
â”‚   â”œâ”€â”€ schemas/    # Pydantic schema validation tests
â”‚   â””â”€â”€ services/   # Business logic unit tests
â”‚       â”œâ”€â”€ browser/
â”‚       â”œâ”€â”€ common/
â”‚       â”œâ”€â”€ crawler/
â”‚       â”œâ”€â”€ proxy/
â”‚       â””â”€â”€ tiktok/
â””â”€â”€ integration/    # Real network/browser tests only
    â”œâ”€â”€ api/        # API integration tests
    â”œâ”€â”€ chromium/   # Browser automation integration
    â”œâ”€â”€ crawl/      # Crawling integration tests
    â”œâ”€â”€ services/   # Service integration tests
    â””â”€â”€ tiktok/     # TikTok integration tests
```

### Test Markers

All test files must have proper pytest markers at the top of the file:
- **Unit tests**: `pytestmark = [pytest.mark.unit]`
- **Integration tests**: `pytestmark = [pytest.mark.integration, pytest.mark.usefixtures("require_scrapling")]`

### Port Information

- Application runs on port 5681 by default
- Health endpoint: `http://localhost:5681/health`
- API docs: `http://localhost:5681/docs`

## Architecture Overview

This is a FastAPI web scraping service using Scrapling for browser automation. The architecture follows a clean layered structure:

### Core Components

- **app/main.py**: Application factory with CORS middleware and lifespan management
- **app/api/routes.py**: REST API endpoints for crawling operations
- **app/core/config.py**: Configuration management using pydantic-settings
- **app/schemas/**: Pydantic models for request/response validation

### Services Layer

- **app/services/crawler/**: Business logic for different scrapers
  - `generic.py`: Generic crawling functionality
  - `dpd.py`: DPD tracking-specific scraping
  - `executors/`: Task execution modules
  - `utils/`: Scraping utilities
- **app/services/tiktok/**: NEW - TikTok session endpoint implementation
  - `service.py`: TikTok session management and login detection
  - `executor.py`: TikTok-specific browsing executor
  - `utils/`: TikTok utilities and login detection logic

### Key Dependencies

- **FastAPI**: Web framework with automatic OpenAPI docs
- **Scrapling**: Browser automation and scraping library
- **BrowserForge**: Browser fingerprinting for anti-detection
- **Pydantic 2.9**: Data validation with Python 3.13 compatibility

### API Endpoints

- `POST /crawl`: Generic crawling endpoint
- `POST /crawl/dpd`: DPD tracking endpoint
- `POST /tiktok/session`: NEW - TikTok interactive session endpoint (requires login)
- `GET /health`: Health check

### Project Structure

```
app/
â”œâ”€â”€ main.py          # FastAPI app factory
â”œâ”€â”€ api/             # HTTP endpoints
â”‚   â”œâ”€â”€ routes.py    # NEW: TikTok session endpoint
â”‚   â””â”€â”€ existing endpoints
â”œâ”€â”€ core/            # Configuration
â”œâ”€â”€ schemas/         # Pydantic models
â”‚   â”œâ”€â”€ tiktok.py    # NEW: TikTok session schemas
â”‚   â””â”€â”€ existing schemas
â”œâ”€â”€ services/        # Business logic
â”‚   â”œâ”€â”€ tiktok/      # NEW: TikTok session implementation
â”‚   â”œâ”€â”€ common/      # NEW: Abstract browsing executor
â”‚   â”œâ”€â”€ crawler/     # Existing crawling services
â”‚   â””â”€â”€ browser/     # Browser automation utilities
â””â”€â”€ middleware/      # Custom middleware
```

### Environment Setup

- Uses `.env` for configuration
- Python 3.10+ required
- Node.js/npm only needed for pm2 production deployment

### Integration Testing

Some tests require real network/browser operations,  which will perform actual web scraping operations.

### Constitutional Requirements

- **Test-Driven Development**: NON-NEGOTIABLE - Tests must be written before implementation
- **Layered Architecture**: Strict separation of API/Core/Middleware/Schemas/Services layers
- **Schema Consistency**: All new schemas must follow existing patterns in `app/schemas/*.py`
- **Code Quality**: Must pass `flake8` linting for `app/` and `tests/` directories
- **Location Rules**: Never place implementation code in `.specify/` directory

## Backwards Compatibility

- **Policy**: NO NEED for backwards compatibility
- **Breaking Changes**: Allowed without migration requirements
- **Focus**: Clean architecture and implementation over legacy support

### CUSTOM RULES

* init.py should be empty

